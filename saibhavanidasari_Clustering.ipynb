{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34ba943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CustomerID', 'CustomerName', 'Region', 'SignupDate'], dtype='object')\n",
      "Index(['TransactionID', 'CustomerID', 'ProductID', 'TransactionDate',\n",
      "       'Quantity', 'TotalValue', 'Price'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(transactions_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print the first few rows of the merged dataframe to confirm the merge result\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the column names and the first few rows of each dataset\n",
    "print(customers_df.columns)\n",
    "print(transactions_df.columns)\n",
    "\n",
    "# Print the first few rows of the merged dataframe to confirm the merge result\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d758bdfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m transactions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUSER\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTransactions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Feature Engineering (merging datasets)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Merge the customers and transactions data on 'CustomerID'\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(transactions, customers, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create features from transaction data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m transaction_summary \u001b[38;5;241m=\u001b[39m merged_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m     21\u001b[0m     TotalValue_sum\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalValue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     22\u001b[0m     TotalValue_mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalValue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     23\u001b[0m     TotalValue_count\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalValue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transactions' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "# Step 2: Load the data\n",
    "customers_df = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\Datasets\\Customers.csv\")\n",
    "transactions_df = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\Datasets\\Transactions.csv\")\n",
    "\n",
    "\n",
    "# Step 3: Feature Engineering (merging datasets)\n",
    "# Merge the customers and transactions data on 'CustomerID'\n",
    "merged_data = pd.merge(transactions, customers, on='CustomerID', how='inner')\n",
    "\n",
    "# Create features from transaction data\n",
    "transaction_summary = merged_data.groupby('CustomerID').agg(\n",
    "    TotalValue_sum=('TotalValue', 'sum'),\n",
    "    TotalValue_mean=('TotalValue', 'mean'),\n",
    "    TotalValue_count=('TotalValue', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Merge transaction features with customer profile\n",
    "final_data = pd.merge(customers, transaction_summary, on='CustomerID', how='inner')\n",
    "\n",
    "# Step 4: Data Normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(final_data[['TotalValue_sum', 'TotalValue_mean', 'TotalValue_count']])\n",
    "\n",
    "# Step 5: KMeans Clustering\n",
    "# Try different values of n_clusters (from 2 to 10)\n",
    "best_db_index = float('inf')\n",
    "best_n_clusters = 2\n",
    "for n_clusters in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    \n",
    "    # Calculate the Davies-Bouldin index for each number of clusters\n",
    "    db_index = davies_bouldin_score(scaled_data, kmeans.labels_)\n",
    "    \n",
    "    if db_index < best_db_index:\n",
    "        best_db_index = db_index\n",
    "        best_n_clusters = n_clusters\n",
    "\n",
    "# Perform final clustering with the best number of clusters\n",
    "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
    "kmeans.fit(scaled_data)\n",
    "final_clusters = kmeans.labels_\n",
    "\n",
    "# Step 6: Cluster Evaluation Metrics\n",
    "sil_score = silhouette_score(scaled_data, final_clusters)\n",
    "print(f'Best number of clusters: {best_n_clusters}')\n",
    "print(f'Davies-Bouldin Index: {best_db_index}')\n",
    "print(f'Silhouette Score: {sil_score}')\n",
    "\n",
    "# Step 7: Visualize Clusters (2D using PCA)\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=final_clusters, cmap='viridis', alpha=0.6)\n",
    "plt.title(f'Customer Segments (KMeans - {best_n_clusters} clusters)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1061ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CustomerID', 'CustomerName', 'Region', 'SignupDate'], dtype='object')\n",
      "Index(['TransactionID', 'CustomerID', 'ProductID', 'TransactionDate',\n",
      "       'Quantity', 'TotalValue', 'Price'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(transactions_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print the first few rows of the merged dataframe to confirm the merge result\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the column names and the first few rows of each dataset\n",
    "print(customers_df.columns)\n",
    "print(transactions_df.columns)\n",
    "\n",
    "# Print the first few rows of the merged dataframe to confirm the merge result\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da8316a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CustomerID TotalValue               \n",
      "                    sum     mean count\n",
      "0      C0001    3354.52  670.904     5\n",
      "1      C0002    1862.74  465.685     4\n",
      "2      C0003    2725.38  681.345     4\n",
      "3      C0004    5354.88  669.360     8\n",
      "4      C0005    2034.24  678.080     3\n"
     ]
    }
   ],
   "source": [
    "# Merging data on CustomerID\n",
    "merged_df = pd.merge(customers_df, transactions_df, on='CustomerID')\n",
    "\n",
    "# Feature engineering: Aggregate transaction features\n",
    "transaction_features = merged_df.groupby('CustomerID').agg({\n",
    "    'TotalValue': ['sum', 'mean', 'count'],  # Use 'TotalValue' instead of 'TransactionAmount'\n",
    "}).reset_index()\n",
    "\n",
    "# Displaying the aggregated features\n",
    "print(transaction_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388d864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CustomerID', 'CustomerName', 'Region', 'SignupDate'], dtype='object')\n",
      "Index(['TransactionID', 'CustomerID', 'ProductID', 'TransactionDate',\n",
      "       'Quantity', 'TotalValue', 'Price'],\n",
      "      dtype='object')\n",
      "  CustomerID      CustomerName         Region  SignupDate TransactionID  \\\n",
      "0      C0001  Lawrence Carroll  South America  2022-07-10        T00015   \n",
      "1      C0001  Lawrence Carroll  South America  2022-07-10        T00932   \n",
      "2      C0001  Lawrence Carroll  South America  2022-07-10        T00085   \n",
      "3      C0001  Lawrence Carroll  South America  2022-07-10        T00445   \n",
      "4      C0001  Lawrence Carroll  South America  2022-07-10        T00436   \n",
      "\n",
      "  ProductID      TransactionDate  Quantity  TotalValue   Price  \n",
      "0      P054  2024-01-19 03:12:55         2      114.60   57.30  \n",
      "1      P022  2024-09-17 09:01:18         3      412.62  137.54  \n",
      "2      P096  2024-04-08 00:01:00         2      614.94  307.47  \n",
      "3      P083  2024-05-07 03:11:44         2      911.44  455.72  \n",
      "4      P029  2024-11-02 17:04:16         3     1300.92  433.64  \n"
     ]
    }
   ],
   "source": [
    "# Print the column names and the first few rows of each dataset\n",
    "print(customers_df.columns)\n",
    "print(transactions_df.columns)\n",
    "\n",
    "# Print the first few rows of the merged dataframe to confirm the merge result\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80774653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CustomerID TotalValue               \n",
      "                    sum     mean count\n",
      "0      C0001    3354.52  670.904     5\n",
      "1      C0002    1862.74  465.685     4\n",
      "2      C0003    2725.38  681.345     4\n",
      "3      C0004    5354.88  669.360     8\n",
      "4      C0005    2034.24  678.080     3\n"
     ]
    }
   ],
   "source": [
    "# Merging data on CustomerID\n",
    "merged_df = pd.merge(customers_df, transactions_df, on='CustomerID')\n",
    "\n",
    "# Feature engineering: Aggregate transaction features\n",
    "transaction_features = merged_df.groupby('CustomerID').agg({\n",
    "    'TotalValue': ['sum', 'mean', 'count'],  # Use 'TotalValue' instead of 'TransactionAmount'\n",
    "}).reset_index()\n",
    "\n",
    "# Displaying the aggregated features\n",
    "print(transaction_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f02318",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m transactions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUSER\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTransactions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Feature Engineering (merging datasets)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Merge the customers and transactions data on 'CustomerID'\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(transactions, customers, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create features from transaction data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m transaction_summary \u001b[38;5;241m=\u001b[39m merged_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m     21\u001b[0m     TotalValue_sum\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalValue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     22\u001b[0m     TotalValue_mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalValue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     23\u001b[0m     TotalValue_count\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalValue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transactions' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
    "\n",
    "# Step 2: Load the data\n",
    "customers_df = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\Datasets\\Customers.csv\")\n",
    "transactions_df = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\Datasets\\Transactions.csv\")\n",
    "\n",
    "\n",
    "# Step 3: Feature Engineering (merging datasets)\n",
    "# Merge the customers and transactions data on 'CustomerID'\n",
    "merged_data = pd.merge(transactions, customers, on='CustomerID', how='inner')\n",
    "\n",
    "# Create features from transaction data\n",
    "transaction_summary = merged_data.groupby('CustomerID').agg(\n",
    "    TotalValue_sum=('TotalValue', 'sum'),\n",
    "    TotalValue_mean=('TotalValue', 'mean'),\n",
    "    TotalValue_count=('TotalValue', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Merge transaction features with customer profile\n",
    "final_data = pd.merge(customers, transaction_summary, on='CustomerID', how='inner')\n",
    "\n",
    "# Step 4: Data Normalization\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(final_data[['TotalValue_sum', 'TotalValue_mean', 'TotalValue_count']])\n",
    "\n",
    "# Step 5: KMeans Clustering\n",
    "# Try different values of n_clusters (from 2 to 10)\n",
    "best_db_index = float('inf')\n",
    "best_n_clusters = 2\n",
    "for n_clusters in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    \n",
    "    # Calculate the Davies-Bouldin index for each number of clusters\n",
    "    db_index = davies_bouldin_score(scaled_data, kmeans.labels_)\n",
    "    \n",
    "    if db_index < best_db_index:\n",
    "        best_db_index = db_index\n",
    "        best_n_clusters = n_clusters\n",
    "\n",
    "# Perform final clustering with the best number of clusters\n",
    "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
    "kmeans.fit(scaled_data)\n",
    "final_clusters = kmeans.labels_\n",
    "\n",
    "# Step 6: Cluster Evaluation Metrics\n",
    "sil_score = silhouette_score(scaled_data, final_clusters)\n",
    "print(f'Best number of clusters: {best_n_clusters}')\n",
    "print(f'Davies-Bouldin Index: {best_db_index}')\n",
    "print(f'Silhouette Score: {sil_score}')\n",
    "\n",
    "# Step 7: Visualize Clusters (2D using PCA)\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=final_clusters, cmap='viridis', alpha=0.6)\n",
    "plt.title(f'Customer Segments (KMeans - {best_n_clusters} clusters)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d35adb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 3. KMeans Clustering\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Applying KMeans clustering\u001b[39;00m\n\u001b[0;32m     30\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(scaled_features)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Adding the cluster labels to the dataframe\u001b[39;00m\n\u001b[0;32m     34\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1471\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1471\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1472\u001b[0m         X,\n\u001b[0;32m   1473\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1474\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[0;32m   1475\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1476\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_x,\n\u001b[0;32m   1477\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1478\u001b[0m     )\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1482\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "customers_df = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\Datasets\\Customers.csv\")\n",
    "transactions_df = pd.read_csv(r\"C:\\Users\\USER\\Desktop\\Datasets\\Transactions.csv\")\n",
    "\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "# Aggregating transaction data for each customer (you can adjust features based on your data)\n",
    "customer_transactions = transactions_df.groupby('CustomerID').agg(\n",
    "    TotalValue_sum=('TotalValue', 'sum'),\n",
    "    TotalValue_mean=('TotalValue', 'mean'),\n",
    "    TotalValue_count=('TotalValue', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Merging the customer profile data with the aggregated transaction data\n",
    "merged_df = pd.merge(customers_df, customer_transactions, on='CustomerID', how='left')\n",
    "\n",
    "# 2. Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(merged_df[['TotalValue_sum', 'TotalValue_mean', 'TotalValue_count']])\n",
    "\n",
    "# 3. KMeans Clustering\n",
    "# Applying KMeans clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "# Adding the cluster labels to the dataframe\n",
    "merged_df['Cluster'] = kmeans.labels_\n",
    "\n",
    "# 4. Dimensionality Reduction (PCA) for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# 5. Visualizing the Clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=merged_df['Cluster'], cmap='viridis', s=50)\n",
    "plt.title('Customer Segments Based on KMeans Clustering')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# 6. Cluster Evaluation (Optional)\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_avg = silhouette_score(scaled_features, kmeans.labels_)\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb69053",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'isnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(scaled_features\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'isnull'"
     ]
    }
   ],
   "source": [
    "print(scaled_features.isnull().sum())  # Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e1968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
